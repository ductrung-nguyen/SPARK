package spark

import collection.immutable.TreeMap

object TestingWorkSheet {

 case class FeatureAggregateInfo(val index: Int, val xValue: Any, var yValue: Double, var frequency: Int) extends Serializable {
        def addFrequency(acc: Int) : FeatureAggregateInfo = { this.frequency = this.frequency + acc; this }
        def +(that : FeatureAggregateInfo) = {
            this.frequency = this.frequency + that.frequency
            this.yValue = this.yValue + that.yValue
            this
        }
        //override def toString() = "index:" + index +  " | xValue:" + xValue + " | yValue" + yValue + " | frequency:" + frequency;
    }
    
    def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ => None }
    def processLine(line: Array[String], numberFeatures: Int): Array[FeatureAggregateInfo] = {
        val length = numberFeatures
        var i = -1;
        parseDouble(line(length - 1)) match {
            case Some(yValue) => {
                line.map(f => {
                    i = (i + 1) % length
                    new FeatureAggregateInfo(i, f, yValue, 1)
                })
            }
            case None => Array[FeatureAggregateInfo]()
        }
        
    }
    
    val dataInputURL = "/home/loveallufev/semester_project/input/small_input2"

    var myDataFile2 = scala.io.Source.fromFile(dataInputURL).getLines.toList

    var mydata = myDataFile2.map(line => line.split(","))
    val number_of_features = mydata.take(1)(0).length
    val temp = mydata.flatMap(processLine(_, number_of_features))
    
    buildTree(temp)
    
    def buildTree(data: List[FeatureAggregateInfo]) : Unit = {
        
        var tmp = (data.groupBy(x => (x.index, x.xValue))
                	.map(x => (x._1._1 ,x._2.foldLeft(new FeatureAggregateInfo(x._1._1, x._1._2, 0,0))(_ + _)))
                )
        tmp.foreach(x => println(x))
    }
    
    
}