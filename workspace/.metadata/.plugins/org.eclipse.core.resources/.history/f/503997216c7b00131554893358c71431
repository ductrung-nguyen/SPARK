package spark

import collection.immutable.TreeMap

object TestingWorkSheet {

	class FeatureAggregateInfo(val index: Int, val xValue: Any, var yValue: Double, var frequency: Int) extends Serializable {
        def addFrequency(acc: Int) : FeatureAggregateInfo = { this.frequency = this.frequency + acc; this }
        def +(that : FeatureAggregateInfo) = {
            this.frequency = this.frequency + that.frequency
            this.yValue = this.yValue + that.yValue
            this
        }
        override def toString() = "Feature(index:" + index +  " | xValue:" + xValue +
        	 " | yValue" + yValue + " | frequency:" + frequency + ")";
    }
    
    def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ => None }
                                                  //> parseDouble: (s: String)Option[Double]
    def processLine(line: Array[String], numberFeatures: Int): Array[FeatureAggregateInfo] = {
        val length = numberFeatures
        var i = -1;
        parseDouble(line(length - 1)) match {
            case Some(yValue) => {
                line.map(f => {
                    i = (i + 1) % length
                    new FeatureAggregateInfo(i, f, yValue, 1)
                })
            }
            case None => Array[FeatureAggregateInfo]()
        }
        
    }                                             //> processLine: (line: Array[String], numberFeatures: Int)Array[spark.TestingW
                                                  //| orkSheet.FeatureAggregateInfo]
    
    val dataInputURL = "/home/loveallufev/semester_project/input/small_input2"
                                                  //> dataInputURL  : java.lang.String = /home/loveallufev/semester_project/input
                                                  //| /small_input2

    var myDataFile2 = scala.io.Source.fromFile(dataInputURL).getLines.toList
                                                  //> myDataFile2  : List[String] = List(hot,sunny,high,false,10,0, hot,sunny,hig
                                                  //| h,true,8,0, hot,overcast,high,false,12,1, cool,rainy,normal,false,14.5,1, c
                                                  //| ool,overcast,normal,true,6.24,1, mild,sunny,high,false,8,0, cool,sunny,norm
                                                  //| al,false,30,1, mild,rainy,normal,false,10,1, mild,sunny,normal,true,1,1, mi
                                                  //| ld,overcast,high,true,7,1, hot,overcast,normal,false,9,1, mild,rainy,high,t
                                                  //| rue,10,0, cool,rainy,normal,true,5,0, mild,rainy,high,false,7,1)

    var mydata = myDataFile2.map(line => line.split(","))
                                                  //> mydata  : List[Array[java.lang.String]] = List(Array(hot, sunny, high, fals
                                                  //| e, 10, 0), Array(hot, sunny, high, true, 8, 0), Array(hot, overcast, high, 
                                                  //| false, 12, 1), Array(cool, rainy, normal, false, 14.5, 1), Array(cool, over
                                                  //| cast, normal, true, 6.24, 1), Array(mild, sunny, high, false, 8, 0), Array(
                                                  //| cool, sunny, normal, false, 30, 1), Array(mild, rainy, normal, false, 10, 1
                                                  //| ), Array(mild, sunny, normal, true, 1, 1), Array(mild, overcast, high, true
                                                  //| , 7, 1), Array(hot, overcast, normal, false, 9, 1), Array(mild, rainy, high
                                                  //| , true, 10, 0), Array(cool, rainy, normal, true, 5, 0), Array(mild, rainy, 
                                                  //| high, false, 7, 1))
    val number_of_features = mydata.take(1)(0).length
                                                  //> number_of_features  : Int = 6
    val temp = mydata.flatMap(processLine(_, number_of_features))
                                                  //> temp  : List[spark.TestingWorkSheet.FeatureAggregateInfo] = List(Feature(in
                                                  //| dex:0 | xValue:hot | yValue0.0 | frequency:1), Feature(index:1 | xValue:sun
                                                  //| ny | yValue0.0 | frequency:1), Feature(index:2 | xValue:high | yValue0.0 | 
                                                  //| frequency:1), Feature(index:3 | xValue:false | yValue0.0 | frequency:1), Fe
                                                  //| ature(index:4 | xValue:10 | yValue0.0 | frequency:1), Feature(index:5 | xVa
                                                  //| lue:0 | yValue0.0 | frequency:1), Feature(index:0 | xValue:hot | yValue0.0 
                                                  //| | frequency:1), Feature(index:1 | xValue:sunny | yValue0.0 | frequency:1), 
                                                  //| Feature(index:2 | xValue:high | yValue0.0 | frequency:1), Feature(index:3 |
                                                  //|  xValue:true | yValue0.0 | frequency:1), Feature(index:4 | xValue:8 | yValu
                                                  //| e0.0 | frequency:1), Feature(index:5 | xValue:0 | yValue0.0 | frequency:1),
                                                  //|  Feature(index:0 | xValue:hot | yValue1.0 | frequency:1), Feature(index:1 |
                                                  //|  xValue:overcast | yValue1.0 | frequency:1), Feature(index:2 | xValue:high 
                                                  //| | yValue1.0 | frequency
                                                  //| Output exceeds cutoff limit.
    
    buildTree(temp)                               //> ((2,normal),Feature(index:2 | xValue:normal | yValue6.0 | frequency:7))
                                                  //| ((1,sunny),Feature(index:1 | xValue:sunny | yValue2.0 | frequency:5))
                                                  //| ((4,14.5),Feature(index:4 | xValue:14.5 | yValue1.0 | frequency:1))
                                                  //| ((3,true),Feature(index:3 | xValue:true | yValue3.0 | frequency:6))
                                                  //| ((3,false),Feature(index:3 | xValue:false | yValue6.0 | frequency:8))
                                                  //| ((0,cool),Feature(index:0 | xValue:cool | yValue3.0 | frequency:4))
                                                  //| ((4,1),Feature(index:4 | xValue:1 | yValue1.0 | frequency:1))
                                                  //| ((2,high),Feature(index:2 | xValue:high | yValue3.0 | frequency:7))
                                                  //| ((0,mild),Feature(index:0 | xValue:mild | yValue4.0 | frequency:6))
                                                  //| ((4,10),Feature(index:4 | xValue:10 | yValue1.0 | frequency:3))
                                                  //| ((4,9),Feature(index:4 | xValue:9 | yValue1.0 | frequency:1))
                                                  //| ((5,0),Feature(index:5 | xValue:0 | yValue0.0 | frequency:5))
                                                  //| ((4,6.24),Feature(index:4 | xValue:6.24 | yValue1.0 | frequency:1))
                                                  //| ((4,8),Feature(index:4 | xValue:8 | yValue0.0 | frequency:2)
                                                  //| Output exceeds cutoff limit.
    
    def buildTree(data: List[FeatureAggregateInfo]) : Unit = {
        
        var tmp = (data.groupBy(x => (x.index, x.xValue))
                	.map(x => (new FeatureAggregateInfo(x._1._1, x._1._2, 0,0) + x._2.foldLeft(new FeatureAggregateInfo(x._1._1, x._1._2, 0,0))(_ + _)))
                )
        tmp.foreach(x => println(x))
        //println(tmp.mkString("***"))
    }                                             //> buildTree: (data: List[spark.TestingWorkSheet.FeatureAggregateInfo])Unit
    
    
}