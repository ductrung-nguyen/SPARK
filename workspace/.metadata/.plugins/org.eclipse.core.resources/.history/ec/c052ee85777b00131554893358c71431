package spark

import collection.immutable.TreeMap
import org.apache.spark._
import org.apache.spark.SparkContext._

object TestingWorkSheet {

    class FeatureAggregateInfo(val index: Int, val xValue: Any, var yValue: Double, var frequency: Int) extends Serializable {
        def addFrequency(acc: Int): FeatureAggregateInfo = { this.frequency = this.frequency + acc; this }
        def +(that: FeatureAggregateInfo) = {
            this.frequency = this.frequency + that.frequency
            this.yValue = this.yValue + that.yValue
            this
        }
        override def toString() = "Feature(index:" + index + " | xValue:" + xValue +
            " | yValue" + yValue + " | frequency:" + frequency + ")";
    }

    case class FeatureSet(file: String, val context: SparkContext) {
        def this(file: String) = this(file, new SparkContext("local", "SparkContext"))
        private def loadFromFile() = {

            //val input_fileName: String = "/home/loveallufev/semester_project/input/small_input";
            val myTagInputFile = context.textFile(file, 1)

            var tags = myTagInputFile.take(2).flatMap(line => line.split(",")).toSeq.toList

            // ( index_of_feature, (Feature_Name, Feature_Type))
            //( (0,(Temperature,1))  , (1,(Outlook,1)) ,  (2,(Humidity,1)) , ... )
            (((0 until tags.length / 2) map (index => (tags(index), tags(index + tags.length / 2)))) zip (0 until tags.length))
                .map(x => FeatureInfo(x._1._1, x._1._2, x._2)).toList
        }

        lazy val data = loadFromFile()
        lazy val numberOfFeature = data.length
    }

    def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ => None }
                                                  //> parseDouble: (s: String)Option[Double]
    def processLine(line: Array[String], numberFeatures: Int, fTypes: Vector[String]): Array[FeatureAggregateInfo] = {
        val length = numberFeatures
        var i = -1;
        parseDouble(line(length - 1)) match {
            case Some(yValue) => { // check type of Y : if isn't continuos type, return nothing
                line.map(f => {
                    i = (i + 1) % length
                    fTypes(i) match {
                        case "0" => {	// If this is a numerical feature => parse value from string to double
                            val v = parseDouble(f);
                            v match {
                                case Some(d) => new FeatureAggregateInfo(i, d, yValue, 1)
                                case None => new FeatureAggregateInfo(-1, f, 0, 0)
                            }
                        }
                        // if this is a categorial feature => return a FeatureAggregateInfo
                        case "1" => new FeatureAggregateInfo(i, f, yValue, 1)
                    }
                })
            }
            case None => Array[FeatureAggregateInfo]()
        }

    }                                             //> processLine: (line: Array[String], numberFeatures: Int, fTypes: Vector[Stri
                                                  //| ng])Array[spark.TestingWorkSheet.FeatureAggregateInfo]

    val context = new SparkContext("local", "SparkContext")
                                                  //> 14/01/12 11:45:47 WARN util.Utils: Your hostname, ubuntu resolves to a loop
                                                  //| back address: 127.0.1.1; using 192.168.190.138 instead (on interface eth0)
                                                  //| 14/01/12 11:45:47 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind t
                                                  //| o another address
                                                  //| 14/01/12 11:45:48 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started
                                                  //| 14/01/12 11:45:48 INFO spark.SparkEnv: Registering BlockManagerMaster
                                                  //| 14/01/12 11:45:48 INFO storage.MemoryStore: MemoryStore started with capaci
                                                  //| ty 390.7 MB.
                                                  //| 14/01/12 11:45:48 INFO storage.DiskStore: Created local directory at /tmp/s
                                                  //| park-local-20140112114548-e19d
                                                  //| 14/01/12 11:45:48 INFO network.ConnectionManager: Bound socket to port 5812
                                                  //| 6 with id = ConnectionManagerId(ubuntu.local,58126)
                                                  //| 14/01/12 11:45:48 INFO storage.BlockManagerMaster: Trying to register Block
                                                  //| Manager
                                                  //| 14/01/12 11:45:48 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Re
                                                  //| gistering block manager ubuntu.local:58126 with
                                                  //| Output exceeds cutoff limit.
    val dataInputURL = "/home/loveallufev/semester_project/input/small_input2"
                                                  //> dataInputURL  : java.lang.String = /home/loveallufev/semester_project/input
                                                  //| /small_input2

    var featureSet = new FeatureSet("/home/loveallufev/semester_project/input/tag_small_input2", context)
                                                  //> featureSet  : spark.TestingWorkSheet.FeatureSet = FeatureSet(/home/loveallu
                                                  //| fev/semester_project/input/tag_small_input2,org.apache.spark.SparkContext@4
                                                  //| c6314d9)

    var myDataFile2 = scala.io.Source.fromFile(dataInputURL).getLines.toList
                                                  //> myDataFile2  : List[String] = List(hot,sunny,high,false,10,0, hot,sunny,hig
                                                  //| h,true,8,0, hot,overcast,high,false,12,1, cool,rainy,normal,false,14.5,1, c
                                                  //| ool,overcast,normal,true,6.24,1, mild,sunny,high,false,8,0, cool,sunny,norm
                                                  //| al,false,30,1, mild,rainy,normal,false,10,1, mild,sunny,normal,true,1,1, mi
                                                  //| ld,overcast,high,true,7,1, hot,overcast,normal,false,9,1, mild,rainy,high,t
                                                  //| rue,10,0, cool,rainy,normal,true,5,0, mild,rainy,high,false,7,1)

    var mydata = myDataFile2.map(line => line.split(","))
                                                  //> mydata  : List[Array[java.lang.String]] = List(Array(hot, sunny, high, fals
                                                  //| e, 10, 0), Array(hot, sunny, high, true, 8, 0), Array(hot, overcast, high, 
                                                  //| false, 12, 1), Array(cool, rainy, normal, false, 14.5, 1), Array(cool, over
                                                  //| cast, normal, true, 6.24, 1), Array(mild, sunny, high, false, 8, 0), Array(
                                                  //| cool, sunny, normal, false, 30, 1), Array(mild, rainy, normal, false, 10, 1
                                                  //| ), Array(mild, sunny, normal, true, 1, 1), Array(mild, overcast, high, true
                                                  //| , 7, 1), Array(hot, overcast, normal, false, 9, 1), Array(mild, rainy, high
                                                  //| , true, 10, 0), Array(cool, rainy, normal, true, 5, 0), Array(mild, rainy, 
                                                  //| high, false, 7, 1))
    val number_of_features = mydata.take(1)(0).length
                                                  //> number_of_features  : Int = 6
    val featureTypes = Vector[String]() ++ featureSet.data.map(x => x.Type)
                                                  //> 14/01/12 11:45:50 INFO storage.MemoryStore: ensureFreeSpace(33744) called w
                                                  //| ith curMem=0, maxMem=409699614
                                                  //| 14/01/12 11:45:50 INFO storage.MemoryStore: Block broadcast_0 stored as val
                                                  //| ues to memory (estimated size 33.0 KB, free 390.7 MB)
                                                  //| 14/01/12 11:45:50 WARN util.NativeCodeLoader: Unable to load native-hadoop 
                                                  //| library for your platform... using builtin-java classes where applicable
                                                  //| 14/01/12 11:45:50 WARN snappy.LoadSnappy: Snappy native library not loaded
                                                  //| 14/01/12 11:45:50 INFO mapred.FileInputFormat: Total input paths to process
                                                  //|  : 1
                                                  //| 14/01/12 11:45:50 INFO spark.SparkContext: Starting job: take at spark.Test
                                                  //| ingWorkSheet.scala:27
                                                  //| 14/01/12 11:45:50 INFO scheduler.DAGScheduler: Got job 0 (take at spark.Tes
                                                  //| tingWorkSheet.scala:27) with 1 output partitions (allowLocal=true)
                                                  //| 14/01/12 11:45:50 INFO scheduler.DAGScheduler: Final stage: Stage 0 (take a
                                                  //| t spark.TestingWorkSheet.scala:27)
                                                  //| 14/01/12 11:45:50 INFO sch
                                                  //| Output exceeds cutoff limit.
    val temp = mydata.flatMap(processLine(_, number_of_features, featureTypes))
                                                  //> temp  : List[spark.TestingWorkSheet.FeatureAggregateInfo] = List(Feature(in
                                                  //| dex:0 | xValue:hot | yValue0.0 | frequency:1), Feature(index:1 | xValue:sun
                                                  //| ny | yValue0.0 | frequency:1), Feature(index:2 | xValue:high | yValue0.0 | 
                                                  //| frequency:1), Feature(index:3 | xValue:false | yValue0.0 | frequency:1), Fe
                                                  //| ature(index:4 | xValue:10.0 | yValue0.0 | frequency:1), Feature(index:5 | x
                                                  //| Value:0.0 | yValue0.0 | frequency:1), Feature(index:0 | xValue:hot | yValue
                                                  //| 0.0 | frequency:1), Feature(index:1 | xValue:sunny | yValue0.0 | frequency:
                                                  //| 1), Feature(index:2 | xValue:high | yValue0.0 | frequency:1), Feature(index
                                                  //| :3 | xValue:true | yValue0.0 | frequency:1), Feature(index:4 | xValue:8.0 |
                                                  //|  yValue0.0 | frequency:1), Feature(index:5 | xValue:0.0 | yValue0.0 | frequ
                                                  //| ency:1), Feature(index:0 | xValue:hot | yValue1.0 | frequency:1), Feature(i
                                                  //| ndex:1 | xValue:overcast | yValue1.0 | frequency:1), Feature(index:2 | xVal
                                                  //| ue:high | yValue1.0 | f
                                                  //| Output exceeds cutoff limit.

    buildTree(temp)                               //> (0,List(Feature(index:0 | xValue:hot | yValue2.0 | frequency:4), Feature(in
                                                  //| dex:0 | xValue:mild | yValue4.0 | frequency:6), Feature(index:0 | xValue:co
                                                  //| ol | yValue3.0 | frequency:4)))
                                                  //| (5,List(Feature(index:5 | xValue:0.0 | yValue0.0 | frequency:5), Feature(in
                                                  //| dex:5 | xValue:1.0 | yValue9.0 | frequency:9)))
                                                  //| (1,List(Feature(index:1 | xValue:sunny | yValue2.0 | frequency:5), Feature(
                                                  //| index:1 | xValue:rainy | yValue3.0 | frequency:5), Feature(index:1 | xValue
                                                  //| :overcast | yValue4.0 | frequency:4)))
                                                  //| (2,List(Feature(index:2 | xValue:high | yValue3.0 | frequency:7), Feature(i
                                                  //| ndex:2 | xValue:normal | yValue6.0 | frequency:7)))
                                                  //| (3,List(Feature(index:3 | xValue:true | yValue3.0 | frequency:6), Feature(i
                                                  //| ndex:3 | xValue:false | yValue6.0 | frequency:8)))
                                                  //| (4,List(Feature(index:4 | xValue:1.0 | yValue1.0 | frequency:1), Feature(in
                                                  //| dex:4 | xValue:5.0 | yValue0.0 | frequency:1), Feature(index:4 | xValue:6.2
                                                  //| 4 | yValue1.0 | fre
                                                  //| Output exceeds cutoff limit.

    def buildTree(data: List[FeatureAggregateInfo]): Unit = {

        var tmp = (data.groupBy(x => (x.index, x.xValue))
            .map(x => (new FeatureAggregateInfo(x._1._1, x._1._2, 0, 0)
                + x._2.foldLeft(new FeatureAggregateInfo(x._1._1, x._1._2, 0, 0))(_ + _)))
            /*
                																	Feature(index:2 | xValue:normal | yValue6.0 | frequency:7)
                                                  Feature(index:1 | xValue:sunny | yValue2.0 | frequency:5)
                                                  Feature(index:4 | xValue:14.5 | yValue1.0 | frequency:1)
                                                  Feature(index:2 | xValue:high | yValue3.0 | frequency:7)
                  */
            .groupBy(x => x.index)
            .map(x =>
            	(x._1, x._2.toList.sortBy(
            		v => v.xValue match {
	                case d: Double => d // sort by xValue if this is numerical feature
	                case s: String => v.yValue / v.frequency // sort by the average of Y if this is categorical value
            		})
            	))
            .map(x => (x._1, x._2))
            )
        tmp.foreach(x => println(x))
        //println(tmp.mkString("***"))
    }                                             //> buildTree: (data: List[spark.TestingWorkSheet.FeatureAggregateInfo])Unit

}